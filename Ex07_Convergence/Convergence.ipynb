{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "speaking-break",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47fe623283acde171dab7629d2b1e1dc",
     "grade": false,
     "grade_id": "cell-1df323fe8f80d873",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercise 07: Undoing temperature\n",
    "===========================================\n",
    "<img src=\"BFnature17440.jpg\" style=\"max-width:45%; float:right; padding-left:30pt\">\n",
    "\n",
    "One of the main ways of probing systems is by measuring absorption spectra,\n",
    "e.g., measuring how much light is absorbed by the material based on its wavelength.\n",
    "\n",
    "On the right you find such a spectrum I took from [Spaun et al. (2016)]: the horizontal axis\n",
    "is the wavelength of the light and the vertical axis is the proportion of the light that\n",
    "is aborbed by the system.\n",
    "Note that as we reduce the temperature, top plot to bottom plot, the main thing that\n",
    "is happening is that the features are getting \"sharper\" and less blurry. (There are\n",
    "population effects as well, but we will ignore those for the moment.)\n",
    "\n",
    "This is because thermal movement causes absorption lines to be less \"precise\", an effect\n",
    "known as [Doppler broadening].  If we denote by $A(\\omega)$ the observed spectrum at\n",
    "given temperature $T$, it is related to the \"true\" spectrum $\\rho(\\omega)$ at absolute zero\n",
    "by:\n",
    "$$\n",
    "    A(\\omega) = \\int d\\omega'\\ K(\\omega - \\omega')\\ \\rho(\\omega'),\n",
    "$$\n",
    "where $K$ is the effect of Doppler broadening.\n",
    "\n",
    "If we assume a set of discrete set of observed frequencies $\\omega_0, \\ldots, \\omega_{N-1}$\n",
    "as well as a discrete set of \"hidden\" frequencies $\\omega'_0, \\ldots, \\omega'_{K-1}$, then\n",
    "this equation becomes:\n",
    "$$\n",
    "    A(\\omega_n) = \\sum_{k=0}^{K-1} K(\\omega_n, \\omega'_k) \\rho(\\omega'_k) + \\epsilon(\\omega_n),\n",
    "$$\n",
    "where $\\epsilon$ is now the inaccuracy in our measurement.  Collecting the spectrums for all\n",
    "frequencies into a vector, we can write this in a very familiar form:\n",
    "$$\n",
    "    \\vec A = K \\vec\\rho + \\vec\\epsilon.\n",
    "$$\n",
    "\n",
    "We would like to \"undo\" the effect of Doppler broadening to get to the true,\n",
    "physical resonances $\\rho$ of our system.  This simply\n",
    "amounts to finding $\\rho$ for a given $A$ and $K$.  This is nothing but linear\n",
    "regression, where $A$ is the labels vector, $K$ is the design matrix, and $\\rho$ are the parameters we are looking for.\n",
    "\n",
    "[Spaun et al. (2016)]: https://www.nature.com/articles/nature17440\n",
    "[Doppler broadening]: https://en.wikipedia.org/wiki/Doppler_broadening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-portable",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6b998f00e3c8497300e915281fb0c3e",
     "grade": false,
     "grade_id": "cell-fbafc306ba676682",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below loads the broadening kernel into `K`, `omega` holds the set of frequencies.  We have measured three spectrums, which we are going to use for training, validation, and testing, respectively: `A_train`, `A_validate`, and `A_test`.\n",
    "\n",
    "Below, please \n",
    "\n",
    " 1. plot each of these spectra over frequency (you can plot in the same figure).\n",
    " 2. make a pseudocolor plot (`pl.pcolor_mesh`) of the broadening kernel. Please add a colorbar. (you may want to pass the option `shading='nearest'`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"broadening.npz\") as _data_file:\n",
    "    omega = _data_file[\"w\"][:400]\n",
    "    K = _data_file[\"K\"][:400, :400]\n",
    "    A_train = _data_file[\"Atrain\"][:400]\n",
    "    A_validate = _data_file[\"Avalidate\"][:400]\n",
    "    A_test = _data_file[\"Atest\"][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-victoria",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c675b64ec1f89c996e9d67455fbfab96",
     "grade": true,
     "grade_id": "cell-0e88e23b16a65956",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot spectra here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-kazakhstan",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49e18018b61c962fd8a9ebc8b0f80bdb",
     "grade": true,
     "grade_id": "cell-bb67d065482b0500",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot kernel here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-telephone",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53b44d2f6c6a5a932dacf310cd23e815",
     "grade": false,
     "grade_id": "cell-4317176a709d37de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Linear model with Gradient descent\n",
    "---------------------\n",
    "As a primer for more complicated methods, let us train this model with gradient\n",
    "descent (SGD).\n",
    "\n",
    "Below you find the code for a gradient descent function `gd_linear` (so that\n",
    "you don't have to write it yet again).  This code trains the parameters of a\n",
    "linear model  starting from $\\vec\\theta = 0$.  Remember, the cost function\n",
    "of such a model is:\n",
    "$$\n",
    "    E(\\vec\\theta) = \\frac 1N \\sum_{n=0}^{N-1} E_n(\\vec\\theta) + f_\\lambda(\\vec\\theta) = \\frac 1N \\sum_{n=0}^{N-1} | \\vec\\theta^T \\vec x_n - y_n |^2 + \\lambda^2 ||\\theta||^2.\n",
    "$$\n",
    "\n",
    "Since we have now written enough Gradient Descent solvers, I have provided\n",
    "both `error_linear`, which computes the cost without regularization, and `gd_step_linear`. This function is a bit special as it performs only a single Gradient descent **step**, taking in the current `theta`, $\\theta^{(t)}$, and returning the next time step, $\\theta^{(t+1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_linear(X, y, theta):\n",
    "    # Compute value of cost function\n",
    "    N, K = X.shape\n",
    "    r = X @ theta - y\n",
    "    return 1/N * (r @ r)\n",
    "\n",
    "def gd_step_linear(X, y, theta, eta=0.1):\n",
    "    \"\"\"Perform single gradient descent step with a linear model.\n",
    "    \n",
    "    Arguments:\n",
    "      - X:         design matrix (observations, features)\n",
    "      - y:         labels\n",
    "      - theta:     parameters to update\n",
    "      - eta:       learning rate\n",
    "    \"\"\"\n",
    "    N, K = X.shape\n",
    "    \n",
    "    # Compute gradient of the cost function\n",
    "    g = 2/N * (X.T @ (X @ theta - y))\n",
    "\n",
    "    # Perform gradient descent step\n",
    "    v = -eta * g\n",
    "    return theta + v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-maryland",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a184a74278ee8a0070eebfe8a8cf839",
     "grade": false,
     "grade_id": "cell-a6fcdb26ae640277",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, use the above functions that fits `rho` from `A_train` using 200 steps of\n",
    "Gradient Descent.  Start from `rho` that is simply a vector of zeros and use a\n",
    "learning rate of `0.1`.\n",
    "\n",
    "You should create three lists of 201 elements each (one for each iteration $t=0,\\ldots,200$):  the list `rhos` contains the current fitted \"unblurred\" spectrum, `E_train` contains the training errors, and `E_validate` contains the validation errors, which are computed from `A_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-startup",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2b193a0dada0c0e1705f4dbe8e3a28c",
     "grade": false,
     "grade_id": "cell-f73c689e8c6c5b67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-authorization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-modeling",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e9b8bf1a0bbcf242c3cf4f913d12105",
     "grade": true,
     "grade_id": "cell-dc07acdd3e4cab84",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (rhos, E_train, E_validate) is not None\n",
    "assert len(rhos) == len(E_train) == len(E_validate) == 201\n",
    "assert rhos[0].shape == omega.shape\n",
    "\n",
    "np.testing.assert_allclose(rhos[0], 0, err_msg=\"Start with zero\")\n",
    "np.testing.assert_array_less(np.diff(E_train), 0, \n",
    "        err_msg=\"Training error must be strictly decreasing for linear models\")\n",
    "np.testing.assert_array_less(np.diff(E_validate[:20]), 0, \n",
    "        err_msg=\"Validation error in this case decreases in the beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-tiger",
   "metadata": {},
   "source": [
    "Plot the training error and the validation error over the iteration number.\n",
    "Use a logarithmic scale for the error.  You also might want to zoom in\n",
    "a little bit (`pl.ylim`) to see the behaviour at larger iteration number\n",
    "more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-checklist",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52e0975a6fb8a5791e3e37617be8a44a",
     "grade": true,
     "grade_id": "cell-f6d542ff9ea4ad2c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "defensive-arrangement",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "335aa4a57b558a4e4ebf10eb004b3d46",
     "grade": false,
     "grade_id": "cell-823c99d555290b37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Observe that the training and validation error behave similarly\n",
    "when plotted over GD iteration than they do when plotted over the\n",
    "number of features. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-moral",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75aaec1e30adf2363fed68a8d6cb2512",
     "grade": true,
     "grade_id": "cell-6516889dc5c80cd1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-travel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "resident-agent",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69748ec00df2a9168475feb9a52e86a8",
     "grade": false,
     "grade_id": "cell-36ee682645ca7d75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Early stopping\n",
    "------------------\n",
    "We performed 200 iterations of Gradient Descent, however it\n",
    "turns out in this case this was wasteful; we could have stopped\n",
    "our procedure earlier.\n",
    "\n",
    "From the variables you recorded in your GD, compute the iteration number\n",
    "where we should have stopped and store it in `iter_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-sense",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b5f5b6f8785291b263d142cb0c6d919",
     "grade": false,
     "grade_id": "cell-b431ab69570dd163",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# iter_opt = ???\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-invasion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-marshall",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf0d999a7c76a19fecf07b382b02462b",
     "grade": true,
     "grade_id": "cell-e87044e25b7c0567",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Just check that the iteration makes *some* sense...\n",
    "assert iter_opt > 10 and iter_opt < 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-google",
   "metadata": {},
   "source": [
    "Now let us analyze how stopping our GD at different iterations influences the shape of our spectra.\n",
    "\n",
    "Plot three spectra (elements of `rhos`): at iteration `2`, at iteration `iter_opt`, and at iteration `200`.  You might want to use subplots here for better visibility: `pl.subplot(311)` etc. should do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-clear",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff3e73d39c59bfbdd5690097378518c9",
     "grade": true,
     "grade_id": "cell-367a9856be7e6a7c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-airplane",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98f2d0d950fa4c06f2e0b7812c3fab7c",
     "grade": false,
     "grade_id": "cell-2dd7010c250800f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us summarize our observations:\n",
    "\n",
    " 1. Why do our spectra get more \"spiky\" as we add more gradient descent steps?\n",
    "   \n",
    " 2. How does early stopping help finding a \"good\" solution here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-onion",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eacdd02d1c2e24b105bb57a95ed16ae3",
     "grade": true,
     "grade_id": "cell-a1c41579c61adea7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-bronze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-glance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-responsibility",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1fc7ce356a3e17685f12fb319b120ae4",
     "grade": false,
     "grade_id": "cell-712ad75d14951950",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Stochastic Gradient Descent\n",
    "----------------------------------------\n",
    "\n",
    "Finally, we are going to analyze the effect of stochasticity.\n",
    "\n",
    "Before we can actually use SGD, we need to make sure our observations are properly randomized.\n",
    "\n",
    "I take care of this for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rng = np.random.default_rng(4711)\n",
    "_perm = _rng.permutation(400)\n",
    "\n",
    "K = K[_perm]\n",
    "A_train = A_train[_perm]\n",
    "A_validate = A_validate[_perm]\n",
    "A_test = A_test[_perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-investment",
   "metadata": {},
   "source": [
    "Next, let us code up stochastic gradient descent.\n",
    "\n",
    "For this, you adapt you `gd_step_linear` to, instead of performing\n",
    "a single gradient descent step, perform a stochastic gradient descent\n",
    "epoch.\n",
    "\n",
    "For this, you divide the observations in `X` and `y` into minibatches\n",
    "of size `M` each.  Then you perform a Gradient Descent step for each\n",
    "of these minibatches.\n",
    "\n",
    "**Hints**: you can reshape the arrays `X` and `y` to add a \"minibatch\"\n",
    "dimension or alternatively use slicing (e.g., `y[0:10]`) to get the\n",
    "minibatch.  You can reuse the function `gd_step_linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-longitude",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015c501aa61f8a2717a816beb9c74bcd",
     "grade": false,
     "grade_id": "cell-0d12fc1db62f4999",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sgd_epoch_linear(X, y, theta, M=2, eta=0.1):\n",
    "    \"\"\"Perform single stochastic gradient descent epoch with a linear model.\n",
    "    \n",
    "    Arguments:\n",
    "      - X:         design matrix (observations, features)\n",
    "      - y:         labels\n",
    "      - theta:     parameters to update\n",
    "      - M:         size of minibatch\n",
    "      - eta:       learning rate\n",
    "    \"\"\"\n",
    "    N, K = X.shape\n",
    "    Nprime = N // M    # number of minibatches\n",
    "    if N % M != 0:\n",
    "        raise ValueError(\n",
    "            \"%d observations cannot be divided into minibatches of size %d\"\n",
    "            % (N, M))\n",
    "    \n",
    "    # Perform a single stochastic Gradient descent epoch\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the trailing ; to see the output\n",
    "sgd_epoch_linear(K, A_train, np.zeros(400), M=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-partner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-doubt",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "982982c4d1a7b6cd2817871b5f36e28b",
     "grade": true,
     "grade_id": "cell-9aca77a6b285ec3d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(4711)\n",
    "_rho0 = rng.normal(size=400)\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    sgd_epoch_linear(K, A_train, _rho0, M=400),\n",
    "    gd_step_linear(K, A_train, _rho0),\n",
    "    err_msg=\"SGD for full batch (M=400) should be same as GD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-assessment",
   "metadata": {},
   "source": [
    "Adapt your code from above that fits `rho` from `A_train` using 200 steps of\n",
    "GD to instead use 200 epochs of stochastic gradient descent with a minibatch\n",
    "size of `M=20` and `eta=0.1`\n",
    "\n",
    "You should again create three lists of 201 elements each (one for each epoch $t=0,\\ldots,200$):  the list `rhos_stoch` contains the current fitted \"unblurred\" spectrum, `E_train_stoch` contains the training errors, and `E_validate_stoch` contains the validation errors, which are computed from `A_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-producer",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f61617eb5bcdbd4b905d9744c7eb233",
     "grade": true,
     "grade_id": "cell-f6b219a05b8b1c01",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-visibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "criminal-comfort",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cb0d31a0c1e8d60b4c54b09895175b7",
     "grade": false,
     "grade_id": "cell-2fcc79de1d29bcb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below, I plot now the errors for the full Gradient Descent (black) and the Stochastic Gradient Descent (red).\n",
    "\n",
    "Answer the following questions below:\n",
    "\n",
    " 1. The training error in SGD is not monotonically falling as in GD. Why?\n",
    " \n",
    " 2. The SGD error saturates at a higher level as the GD error. Why?\n",
    " \n",
    " 3. While for GD, the training and validation error are drifting apart for\n",
    "    large iteration number, this seems not to be the case for SGD. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.semilogy(E_train, '--k', label='GD: training error')\n",
    "pl.semilogy(E_validate, '-k', label='GD: validation error ')\n",
    "pl.plot(E_train_stoch, '--r', label='SGD: training error')\n",
    "pl.plot(E_validate_stoch, '-r', label='SGD: validation error')\n",
    "pl.xlabel('iteration')\n",
    "pl.ylabel('$E/N$')\n",
    "pl.xlim(-1, 201)\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-saint",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed0ebea52b8ec9ffd493c76c4acbcbd9",
     "grade": true,
     "grade_id": "cell-519e151ff6b3ac9f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-failure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-somewhere",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
